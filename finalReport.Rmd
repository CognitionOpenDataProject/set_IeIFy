---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: IeIFy
#### Pilot: Sara Kessler
#### Co-pilot: Tom Hardwicke  
#### Start date: Mar 20 2017
#### End date: Oct 5 2017
#### Final verification: Tom Hardwicke
#### Date: Nov 9 2017

-------

#### Methods summary: 

Arnold et al. (2016) investigated whether temporal information from previous experiences is retained in mental simulations, hypothesizing that "simulated episodes contain temporal aspects of the experiences the simulation is recapitulated from, albeit in a compressed form" (Arnold et al., 2016, p. 15). In order to investigate this question, participants were shown a video from a first person perspective of a walk around the perimeter of a virtual city which had five visually salient landmarks in it. The landmarks were pointed out and introduced. Participants then underwent 20 encoding trials in which they had to navigate between two landmarks, as quickly as possible. Finally, the participants had a simulation phase where they were shown images of two landmarks and then had to close their eyes and mentally simulate navigating from one to the other, imagining their route in detail.  They were asked to use the quickest route possible, not necessarily one they had used in the encoding phase. After the mental simulation they answered a questionnaire to probe qualitative aspects of the simulations. Then they had to navigate the actual route in the virtual city as quickly as possible, followed by another questionnaire about how well they navigated the route, and how closely their simulation matched the actual navigation. There were 10 routes in the simulation phase. 

------

#### Target outcomes: 
The target outcomes for this article are outlined in Section 2.1.3 of Arnold et al. (2016):

> Inspection of the route time histogram from the simulation phase revealed a number of trials in which participants became “lost” (see Fig. S1), which skewed the distribution. To control for this, we calculated the difference between the optimal route time and the observed route time (mean difference score = 14.27s; SD = 19.49). The resulting distribution was then used to remove trials in the top 25% of difference scores across participants (75% quartile = 21.66 s, 70 trials removed). This filtering strategy allowed for retention of variance in route time, while excluding trials that took approximately double the optimal route time (M = 24.85 s, SD = 7.16). Route performance on the filtered data set of 210 trials was near the optimal route time (mean difference score = 5.93 s, SD = 3.54).

> Next, we assessed the relationship between simulation time (M = 14.41, SD = 11.21) and route navigation time (M = 35.81, SD = 9.25; see Fig. 2a–b). Simulation times were first mean-centered for each participant, providing a more precise estimate of coefficients as it minimizes variance in simulation time due to individual differences in overall temporal compression rate. We found a statistically significant positive correlation between the time it took a participant to subsequently navigate the route and the time it took them to mentally simulate it (r(208) = 0.30, p < 0.001, R2 = 0.09, Fig. 3C). We also found a significant positive correlation between simulation time and route distance (r(208) = 0.29, p < 0.001, R2 = 0.08); however, route time and distance for each trial were highly collinear (r(208) = 0.97, p < 0.001, R2 = 0.94). As such, the remainder of the analysis focuses on route time, which accounts for variance in non-movement related processes (e.g. making decisions at turning points) that are not represented in the distance measure. Our correlation reported here between simulation and route time is consistent with past findings from Kosslyn et al. (1978) who showed a correlation between the time it took participants to mentally scan between different locations on a map of an island and the physical distance between them.

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)

# prepare an empty report object, we will update this each time we run compareValues2()
reportObject <- data.frame("Article_ID" = NA, "valuesChecked" = 0, "eyeballs" = 0, "Total_df" = 0, "Total_p" = 0, "Total_mean" = 0, "Total_sd" = 0, "Total_se" = 0, "Total_ci" = 0, "Total_bf" = 0, "Total_t" = 0, "Total_F" = 0, "Total_es" = 0, "Total_median" = 0, "Total_irr" = 0, "Total_r" = 0, "Total_z" = 0, "Total_coeff" = 0, "Total_n" = 0, "Total_x2" = 0, "Total_other" = 0, "Insufficient_Information_Errors" = 0, "Decision_Errors" = 0, "Major_Numerical_Errors" = 0, "Minor_Numerical_Errors" = 0, "Major_df" = 0, "Major_p" = 0, "Major_mean" = 0, "Major_sd" = 0, "Major_se" = 0, "Major_ci" = 0, "Major_bf" = 0, "Major_t" = 0, "Major_F" = 0, "Major_es" = 0, "Major_median" = 0, "Major_irr" = 0, "Major_r" = 0, "Major_z" = 0, "Major_coeff" = 0, "Major_n" = 0, "Major_x2" = 0, "Major_other" = 0, "affectsConclusion" = NA, "error_typo" = 0, "error_specification" = 0, "error_analysis" = 0, "error_data" = 0, "error_unidentified" = 0, "Author_Assistance" = NA, "resolved_typo" = 0, "resolved_specification" = 0, "resolved_analysis" = 0, "resolved_data" = 0, "correctionSuggested" = NA, "correctionPublished" = NA)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
sem <- function(x) {sd(x, na.rm=TRUE) / sqrt(length(x))} # custom function to calculate standard error of the mean
ci95 <- function(x) {sem(x) * 1.96} # custom function to calculate 95% confidence intervals
```

## Step 2: Load data

```{r}
d_raw <- read_csv("data/data3.csv")
d_filtered <- read_csv("data/data1.csv")
```

## Step 3: Tidy data

```{r}
#The optimal times were copied out of data_1 for subject 26 in group 1 who had none of their trials excluded
opt_times <- d_filtered %>%
  filter(Group == 1, Participant == 26) %>%
  select(Optimal)
num_subs <- length(unique(d_raw$Participant))

d_tidy <- d_raw %>%
  rename(trial_num = X1, 
         obs_route_time = Path_Time, 
         distance = Distance, 
         sim_time = Sim_Time, 
         subid = Participant) %>%
  select(subid, trial_num, obs_route_time, distance, sim_time)%>%
  mutate(optimal_route_time = rep(opt_times$Optimal,num_subs))
```

## Step 4: Run analysis

### Pre-processing

Let's try and reproduce these outcomes:

> Inspection of the route time histogram from the simulation phase revealed a number of trials in which participants became “lost” (see Fig. S1), which skewed the distribution. To control for this, we calculated the difference between the optimal route time and the observed route time (mean difference score = 14.27s; SD = 19.49).

```{r}
d <- d_tidy %>%
  mutate(diff = obs_route_time - optimal_route_time)

summary(d$diff)
mean_diff <- mean(d$diff)
sd_diff <- sd(d$diff)

reportObject <- compareValues2(reportedValue = "14.27", obtainedValue = mean_diff, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "19.49", obtainedValue = sd_diff, valueType = 'sd')
```

These summary statistics appear to differ substantially from the values reported in the article, but DO match the information reported in the analysis script provided in the supplementary materials as this screenshot shows:

![](images/supMat_1.png)

Let's plot a histogram for the distribution of difference scores:

```{r}
ggplot(d, aes(x=diff)) +
    geom_histogram(binwidth=20, boundary = 0, colour="black", fill="white")
```

and compare to supplementary figure 1 from the original article:

![](images/fig_s1.png)

The distributions appear to be very similar but the first two bins are different. We have used bins of 20 as the code in the supplementary material appears to indicate:

> sns.distplot(p_data.Diff.dropna(), kde=False, bins=20);

So it is unclear why there is a difference.

Now we look at the filtering strategy of removing the top 25% of difference scores:

> The resulting distribution was then used to remove trials in the top 25% of difference scores across participants (75% quartile = 21.66 s, 70 trials removed). 

```{r}
# filter out the top 25% of difference times
top_quant <- quantile(d$diff, .75) # identify cut-off for top 25% quantile
reportObject <- compareValues2(reportedValue = "21.66", obtainedValue = top_quant, valueType = 'other')

d_topFilter <- d %>%
  filter(d$diff < top_quant) # apply filter

n_prefilter <- nrow(d) # number of rows pre-filter
n_postfilter <- nrow(d_topFilter) # number of rows remaining.
n_removed <- n_prefilter - n_postfilter # number of rows removed
reportObject <- compareValues2(reportedValue = "70", obtainedValue = n_removed, 'other')
```

The top quantile matches what was reported in the paper, and the same number of trials were filtered out using this criterion. Strangely it does not match what was reported in the text of the supplementary materials:

> First, we're going to remove the top quartile, which means any trial with a difference score that exceeds 14.95 seconds.

...but does match the value shown in the descriptive statistics screenshot shown above (i.e., 75% quantile = 21.66).


### Descriptive statistics

Let's try to reproduce these target outcomes:

> This filtering strategy allowed for retention of variance in route time, while excluding trials that took approximately double the optimal route time (M = 24.85 s, SD = 7.16).

```{r}
mean_opt = mean(opt_times$Optimal)
sd_opt = sd(opt_times$Optimal)

reportObject <- compareValues2(reportedValue = "24.85", obtainedValue = mean_opt, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "7.16", obtainedValue = sd_opt, valueType = 'sd')
```

Looks fine. Now let's try to reproduce these target outcomes:

> Route performance on the filtered data set of 210 trials was near the optimal route time (mean difference score = 5.93 s, SD = 3.54).

```{r}
mean_diff_filt <- mean(d_topFilter$diff)
sd_diff_filt <- sd(d_topFilter$diff)

reportObject <- compareValues2(reportedValue = "5.93", obtainedValue = mean_diff_filt, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "3.54", obtainedValue = sd_diff_filt, valueType = 'sd')
```

There is a discrepancy in the mean and sd of difference scores. Again, the values we obtained in our reanalysis appear to match values reported in the supplementary materials (Table 3) but not in the main article.

Now let's try and reproduce the following target outcomes:

> Next, we assessed the relationship between simulation time (M = 14.41, SD = 11.21)...

```{r}
mean_sim_time <- mean(d_topFilter$sim_time)
reportObject <- compareValues2(reportedValue = "14.41", obtainedValue = mean_sim_time, valueType = 'mean')

sd_sim_time <- sd(d_topFilter$sim_time)
reportObject <- compareValues2(reportedValue = "11.21", obtainedValue = sd_sim_time, valueType = 'sd')
```

> and route navigation time (M = 35.81, SD = 9.25; see Fig. 2a–b). 

```{r}
mean_nav_time <- mean(d_topFilter$obs_route_time)
reportObject <- compareValues2(reportedValue = "35.81", obtainedValue = mean_nav_time, valueType = 'mean')

sd_nav_time <- sd(d_topFilter$obs_route_time)
reportObject <- compareValues2(reportedValue = "9.25", obtainedValue = sd_nav_time, valueType = 'sd')
```

The reported mean and SD for simulation time and route navigation times match those that we found here.

```{r}
d_g1<-d_topFilter %>%
  mutate(group = 1) %>%
  group_by(group) %>%
  summarize(mean = mean(obs_route_time),
            cis = ci95(obs_route_time))
ggplot(data=d_g1, aes(x=group, y=mean)) +
    geom_bar(stat="identity", fill="#FF6600") +
    geom_errorbar(aes(ymin=mean-cis, ymax=mean+cis),
                  width=.2, position=position_dodge(.9)) + xlab("Medium") + ylab("Seconds") + ggtitle("Route Time") +
  scale_x_discrete(breaks=NULL)
```

```{r}
d_g2<-d_topFilter %>%
  mutate(group = 1) %>%
  group_by(group) %>%
  summarize(mean = mean(sim_time),
            cis = ci95(sim_time))
ggplot(data=d_g2, aes(x=group, y=mean)) +
    geom_bar(stat="identity", fill="#FF6600") +
    geom_errorbar(aes(ymin=mean-cis, ymax=mean+cis),
                  width=.2, position=position_dodge(.9)) + xlab("Medium") + ylab("Seconds") + ggtitle("Simulation Time") +
  scale_x_discrete(breaks=NULL)
```



![Figure 2](images/fig_2.png)

Our figures seem to match the middle (orange) bars of Figures 2a and 2b.

> Simulation times were first mean-centered for each participant, providing a more precise estimate of coefficients as it minimizes variance in simulation time due to individual differences in overall temporal compression rate.

```{r}
#mean center the simulation times for each participant then rejoin them with the rest of the data.
d_centered <- d_topFilter %>%
  group_by(subid)%>%
  mutate(centered_sim_time = scale(sim_time, center = T, scale = F)[,1])%>%
  group_by(subid, trial_num, add = FALSE) %>%
  summarize(centered_sim_time = mean(centered_sim_time))

d_topFilter <- right_join(d_topFilter, d_centered)
```


### Inferential statistics

> We found a statistically significant positive correlation between the time it took a participant to subsequently navigate the route and the time it took them to mentally simulate it (r(208) = 0.30, p < 0.001, R2 = 0.09, Fig. 3C).

```{r}
cor.out <- cor.test(d_topFilter$obs_route_time, d_topFilter$centered_sim_time)

reportObject <- compareValues2(reportedValue = "208", obtainedValue = cor.out$parameter, valueType = 'df') # df
reportObject <- compareValues2(reportedValue = ".30", obtainedValue = cor.out$estimate, valueType = 'r') # r
reportObject <- compareValues2(reportedValue = ".09", obtainedValue = cor.out$estimate^2, valueType = 'other') # r2
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor.out$p.value, valueType = 'p') # p
```
The exact p-value is not reported in the paper, but by eyeballing we can see that the obtained value matches the reported boundary.

Using the centered simulation times for each participant, we found the same significant positive correlation between the time it took a participant to subsequently navigate the route and the time it took them to mentally simulate it as was found in the paper.

> We also found a significant positive correlation between simulation time and route distance (r(208) = 0.29, p < 0.001, R2 = 0.08);

```{r}
cor.out <- cor.test(d_topFilter$distance, d_topFilter$centered_sim_time)

reportObject <- compareValues2(reportedValue = "208", obtainedValue = cor.out$parameter, valueType = 'df') # df
reportObject <- compareValues2(reportedValue = "0.29", obtainedValue = cor.out$estimate, valueType = 'r') # r
reportObject <- compareValues2(reportedValue = "0.08", obtainedValue = cor.out$estimate^2, valueType = 'other') # r2
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor.out$p.value, valueType = 'p') # p
```
The exact p-value is not reported in the paper, but by eyeballing we can see that the obtained value matches the reported boundary.

> however, route time and distance for each trial were highly collinear (r(208) = 0.97, p < 0.001, R2 = 0.94). As such, the remainder of the analysis focuses on route time, which accounts for variance in non-movement related processes (e.g. making decisions at turning points) that are not represented in the distance measure. Our correlation reported here between simulation and route time is consistent with past findings from Kosslyn et al. (1978) who showed a correlation between the time it took participants to mentally scan between different locations on a map of an island and the physical distance between them.

```{r}
cor.out <- cor.test(d_topFilter$distance, d_topFilter$obs_route_time)

reportObject <- compareValues2(reportedValue = "208", obtainedValue = cor.out$parameter, valueType = 'df') # df
reportObject <- compareValues2(reportedValue = ".97", obtainedValue = cor.out$estimate, valueType = 'r') # r
reportObject <- compareValues2(reportedValue = ".94", obtainedValue = cor.out$estimate^2, valueType = 'other') # r2
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = cor.out$p.value, valueType = 'p') # p
```

The exact p-value is not reported in the paper, but by eyeballing we can see that the obtained value matches the reported boundary.

## Step 5: Conclusion

We found that the inferential statistics were congruent with those reported in the paper. However, there were inconsistencies with some of the descriptive statistics reported in the paper. The mean and SD of the difference score between the optimal route time and the observed route time were different, as were the mean and SD of the difference score of the filtered data. 

We contacted the original authors for clarification and they confirmed that the aforementioned values reported in the article are incorrect. The values obtained in our reanalysis, and reported in the original supplementary materials, are the correct values. The authors suggested that this error arose because they did not remember to update the manuscript after updating some aspects of the analysis. The authors have indicated that they will contact the journal to correct these errors. Note that the errors do not appear to impact upon the substantive statistical conclusions of the article.

```{r}
reportObject$Article_ID <- "IeIFy"
reportObject$affectsConclusion <- "no"
reportObject$error_typo <- 0
reportObject$error_specification <- 0
reportObject$error_analysis <- 0
reportObject$error_data <- 0
reportObject$error_unidentified <- 0
reportObject$Author_Assistance <- T
reportObject$resolved_typo <- 0
reportObject$resolved_specification <- 0
reportObject$resolved_analysis <- 1
reportObject$resolved_data <- 0
reportObject$correctionSuggested <- T
reportObject$correctionPublished <- F

# decide on final outcome
if(reportObject$Decision_Errors > 0 | reportObject$Major_Numerical_Errors > 0 | reportObject$Insufficient_Information_Errors > 0){
  reportObject$finalOutcome <- "Failure"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Failure despite author assistance"
  }
}else{
  reportObject$finalOutcome <- "Success"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Success with author assistance"
  }
}

# save the report object
filename <- paste0("reportObject_", reportObject$Article_ID,".csv")
write_csv(reportObject, filename)
```

## Report Object

```{r, echo = FALSE}
# display report object in chunks
kable(reportObject[2:10], align = 'l')
kable(reportObject[11:20], align = 'l')
kable(reportObject[21:25], align = 'l')
kable(reportObject[26:30], align = 'l')
kable(reportObject[31:35], align = 'l')
kable(reportObject[36:40], align = 'l')
kable(reportObject[41:45], align = 'l')
kable(reportObject[46:51], align = 'l')
kable(reportObject[52:57], align = 'l')
```

## Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
